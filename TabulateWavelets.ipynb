{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b40c32a2-cc99-4778-b046-25a84b10db6f",
   "metadata": {},
   "source": [
    "# Import libraries and load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "79fc9451-4499-4b00-bc55-cb99943f93d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard Libraries #\n",
    "import pandas as pd\n",
    "#import numpy as np\n",
    "import datetime\n",
    "from datetime import timedelta\n",
    "import pathlib\n",
    "import h5py\n",
    "from redcap import Project\n",
    "\n",
    "# Custom functions #\n",
    "from Tools import GetNeuropaceCatalog, GetChannelAxis, GetPeakFrequencies, GetWaveletsArray, GetTimeAxis, TabulateWavelets, GetArtifactTags, format_time\n",
    "\n",
    "pd.set_option('display.max_rows', 100) #set max of display rows when printing dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d357eaa7-0bea-45ba-b334-e9e7c0f11665",
   "metadata": {},
   "outputs": [],
   "source": [
    "# User-specified inputs #\n",
    "patient_id = 'PR05'\n",
    "api_key = '51E24C4540ED17AE20E61248F71B4F84' #API token assigned to unique user AND project by REDCap administrator (to request: log in with credentials on REDCap, go to a project, click API under Applications) \n",
    "output_dir = f'/userdata/dastudillo/patient_data/stage2/{patient_id}'\n",
    "\n",
    "magnet_on    = True\n",
    "scheduled_on = False\n",
    "realtime_on  = False\n",
    "\n",
    "period = 1/250 #250 Hz is sampling frequency set on RNS devices\n",
    "artifact_version = 'stimartifact_v2' #change according to artifact dataset name within hdf5 you want to use\n",
    "\n",
    "start, stop = '2024-09-06', '2024-09-07'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "691df810-7339-4f90-9f46-89a686d1ad23",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Export datasets; surveys exported from REDCap via API, ecog catalog and preprocessed ecog files (HDF5) exported from server\n",
    "api_url = 'https://redcap.ucsf.edu/api/'\n",
    "surveys_raw = Project(api_url, api_key).export_records(format_type='df').reset_index()\n",
    "rns_raw = GetNeuropaceCatalog(patient_id)\n",
    "\n",
    "# Read HDF5 file containing preprocessed data #\n",
    "PREPROC_DIR = f'/data_store0/presidio/NeuroPace_DB/{patient_id}/NP_PROC_HDF/signals_db-pipeline_d8bc5025145576d05d1934d0f027bbb1.hdf5'\n",
    "PREPROC_DATA = h5py.File(PREPROC_DIR, 'r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0caef33-e15c-4e46-997e-a6929387b9a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up and extract relevant columns from rns catalog # \n",
    "# Do not modify\n",
    "rns_raw = rns_raw.rename(columns={'ECoG trigger':'TriggerType',\n",
    "                                  'Timestamp':'Start_Timestamp_PT',\n",
    "                                  'Raw local timestamp':'Trigger_Timestamp_Local',\n",
    "                                  'ECoG length':'Total_Duration',\n",
    "                                  'ECoG pre-trigger length':'PreTrigger_Duration'})\n",
    "\n",
    "if (magnet_on==True) & (scheduled_on==False) & (realtime_on==False):\n",
    "    mask = (rns_raw.TriggerType=='Magnet')\n",
    "if (magnet_on==True) & (scheduled_on==True) & (realtime_on==False):\n",
    "    mask = ((rns_raw.TriggerType=='Magnet') ^ (rns_raw.TriggerType=='Scheduled'))\n",
    "if (magnet_on==False) & (scheduled_on==False) & (realtime_on==True):\n",
    "    mask = (rns_raw.TriggerType=='Real_Time')\n",
    "\n",
    "rns_clean = rns_raw.loc[mask, ['Filename', 'TriggerType', 'Trigger_Timestamp_Local', 'PreTrigger_Duration', 'Total_Duration']].reset_index(drop=True)\n",
    "rns_clean['Trigger_Timestamp_Local'] = pd.to_datetime(rns_clean['Trigger_Timestamp_Local'], format='%Y-%m-%d %H:%M:%S.%f')\n",
    "\n",
    "start_timestamp_local = []\n",
    "for i, j in zip(rns_clean.Trigger_Timestamp_Local, rns_clean.PreTrigger_Duration):\n",
    "    start_timestamp_local.append(i - timedelta(seconds=j))\n",
    "rns_clean.insert(2, 'Start_Timestamp_Local', start_timestamp_local)\n",
    "\n",
    "stop_timestamp_local = []\n",
    "for i, j in zip(rns_clean.Start_Timestamp_Local, rns_clean.Total_Duration):\n",
    "    stop_timestamp_local.append(i + timedelta(seconds=j))\n",
    "rns_clean.insert(3, 'Stop_Timestamp_Local', stop_timestamp_local)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ba368f2-b1af-4ef3-8279-4e7aaa62df0c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#filter files according to date range specified in user inputs. \n",
    "mask = (rns_clean.Trigger_Timestamp_Local>=datetime.datetime.strptime(start, '%Y-%m-%d')) & (rns_clean.Trigger_Timestamp_Local<=datetime.datetime.strptime(stop, '%Y-%m-%d'))\n",
    "rns_filter = rns_clean.loc[mask].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e89d1c00-3465-4ede-9d07-3f4e025ba534",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get list of file names and start timestamps of recordings\n",
    "files       = rns_filter.Filename\n",
    "files_start = rns_filter.Start_Timestamp_Local\n",
    "files_triggertype = rns_filter.TriggerType\n",
    "files_triggertimestamp = rns_filter.Trigger_Timestamp_Local\n",
    "print('Number of files: ', len(files))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b72db73-1c2d-4b9c-9410-34dc96afe10c",
   "metadata": {},
   "source": [
    "# Tabulate preprocessed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea54af9b-90c5-4c1b-8617-814447ed1333",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use custom functions to get wavelets tabulated\n",
    "#NOTE: Suuuper time consuming, don't use to tabulate large number of files (max 5), warning: huge dataframe!!\n",
    "#TO DO: adapt script to send as server job\n",
    "\n",
    "allfiles_df = pd.DataFrame()\n",
    "\n",
    "for i in range(len(files)):\n",
    "    channels = GetChannelAxis(PREPROC_DATA[files[i]])\n",
    "    peak_freqs = GetPeakFrequencies(PREPROC_DATA[files[i]])\n",
    "    wavelets = GetWaveletsArray(PREPROC_DATA[files[i]])\n",
    "    n_samples = wavelets.shape[2]\n",
    "    time_axis = GetTimeAxis(PREPROC_DATA[files[i]], n_samples, files_start[i])\n",
    "\n",
    "    artifact_tags = GetArtifactTags(PREPROC_DATA[files[i]], files_start[i], period, artifact_version)\n",
    "    artifact_tags['ArtifactTimestamp'] = pd.to_datetime(pd.Series(format_time(artifact_tags.ArtifactTimestamp)))\n",
    "\n",
    "    wavelets_ch1 = TabulateWavelets(channels[0], wavelets[0], peak_freqs, time_axis)\n",
    "    wavelets_ch2 = TabulateWavelets(channels[1], wavelets[1], peak_freqs, time_axis)\n",
    "    wavelets_ch3 = TabulateWavelets(channels[2], wavelets[2], peak_freqs, time_axis)\n",
    "    wavelets_ch4 = TabulateWavelets(channels[3], wavelets[3], peak_freqs, time_axis)\n",
    "    all_wavelets = pd.concat([wavelets_ch1, wavelets_ch2, wavelets_ch3, wavelets_ch4], axis=1).T.drop_duplicates().T\n",
    "    all_wavelets['Timestamp'] = pd.to_datetime(all_wavelets['Timestamp'])\n",
    "    del wavelets_ch1, wavelets_ch2, wavelets_ch3, wavelets_ch4\n",
    "    del wavelets\n",
    "\n",
    "    file_df = all_wavelets.merge(artifact_tags, how='inner', left_on='Timestamp', right_on='ArtifactTimestamp')\n",
    "    file_df['Filename'] = files[i]\n",
    "    file_df['TriggerType'] = files_triggertype[i]\n",
    "    file_df['TriggerLocalTimestamp'] = files_triggertimestamp[i]\n",
    "    \n",
    "    col0 = file_df.pop('Filename')\n",
    "    col1 = file_df.pop('TriggerType')\n",
    "    col2 = file_df.pop('TriggerLocalTimestamp')\n",
    "    \n",
    "    file_df.insert(0, 'Filename', col0)\n",
    "    file_df.insert(1, 'TriggerType', col1)\n",
    "    file_df.insert(2, 'TriggerLocalTimestamp', col2)\n",
    "    \n",
    "    del artifact_tags\n",
    "    print(f'Adding file {i+1} to final dataframe')\n",
    "    print('')\n",
    "    \n",
    "    allfiles_df = pd.concat([allfiles_df, file_df], ignore_index=True)\n",
    "\n",
    "PREPROC_DATA.close() #close hdf5 file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cb83835-7954-4677-a3f9-106867f3413a",
   "metadata": {},
   "outputs": [],
   "source": [
    "allfiles_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd82a264-4b1b-45fc-a68c-a3a7f1b7bd7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "allfiles_df.to_csv(pathlib.Path(output_dir, f'{patient_id}_Stage2_wavelets_{start}_{stop}.csv'), index=False)\n",
    "del allfiles_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
